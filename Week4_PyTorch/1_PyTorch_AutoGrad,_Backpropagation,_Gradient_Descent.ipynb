{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["PyTorch is an open-source deep learning framework developed by Facebook (Meta).\n","\n","PyTorch is a popular open-source deep learning framework known for its flexibility, dynamic computation graphs, and ease of use, making it a favorite for research and development.\n","Popular in research, academic settings, and prototyping due to intuitive syntax and debugging ease.\n","\n","Tensors in PyTorch are similar to NumPy arrays but support GPU acceleration.\n","\n","Use PyTorch if you want flexibility, ease of debugging, and research-oriented training.\n","\n","Use TensorFlow if you prefer built-in training routines and large-scale production deployment\n","\n"],"metadata":{"id":"WL19HFeAT1tg"}},{"cell_type":"markdown","source":["Unlike TensorFlow, PyTorch tensors are mutable, meaning they can be modified in place"],"metadata":{"id":"OkJbXq7RVWt-"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Create a TensorFlow tensor\n","x = tf.constant([1, 2, 3])\n","\n","# Attempting to modify the tensor (This will cause an error!)\n","# x[0] = 10  #  TensorFlow tensors are immutable\n","\n","# Instead, you must create a new tensor\n","x_new = x + 5  # Adds 5 to all elements, but creates a new tensor\n","\n","print(x_new)  # Output: tf.Tensor([6 7 8], shape=(3,), dtype=int32)\n","\n","\n","\n","\n","import torch\n","\n","# Create a PyTorch tensor\n","x = torch.tensor([1, 2, 3])\n","\n","# Modify the tensor in-place\n","x[0] = 10  # Changing the first element\n","x += 5     # Adding 5 to all elements\n","\n","print(x)  # Output: tensor([15, 7, 8])"],"metadata":{"id":"XJBfDq6MUYsW","executionInfo":{"status":"ok","timestamp":1745893276306,"user_tz":-330,"elapsed":17424,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}},"outputId":"aab7e85e-5685-4317-eb07-8cdd37756f56","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([6 7 8], shape=(3,), dtype=int32)\n","tensor([15,  7,  8])\n"]}]},{"cell_type":"markdown","source":["Both PyTorch and TensorFlow support automatic differentiation, but PyTorch makes it easier and more intuitive using autograd."],"metadata":{"id":"gCqqaVmuVRSx"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Create a TensorFlow variable\n","x = tf.Variable(2.0)\n","\n","with tf.GradientTape() as tape:                    #use tf.GradientTape() to record operations\n","    y = x ** 3  # Define function y = x^3\n","\n","# Compute gradient dy/dx\n","grad = tape.gradient(y, x)\n","\n","# Print gradient (dy/dx = 3x^2 at x=2)\n","print(grad.numpy())  # Output: 12.0\n","\n","\n","import torch\n","\n","# Create a tensor with requires_grad=True to track gradients\n","x = torch.tensor(2.0, requires_grad=True)\n","\n","# Define function y = x^3\n","y = x ** 3\n","\n","# Compute gradient dy/dx\n","y.backward()\n","\n","# Print gradient (dy/dx = 3x^2 at x=2)\n","print(x.grad)  # Output: tensor(12.)\n"],"metadata":{"id":"3hqd282RU_zJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745893277888,"user_tz":-330,"elapsed":1567,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}},"outputId":"48c1da85-4399-4d31-ef57-00655e7e21a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12.0\n","tensor(12.)\n"]}]},{"cell_type":"markdown","source":[" Working with GPU in PyTorch vs. TensorFlow\n","\n","\n","TensorFlow: Automatically assigns operations to available GPUs.\n","\n","PyTorch: Requires explicit tensor and model assignment to a GPU."],"metadata":{"id":"utxmRiOrWFq2"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Check available GPUs\n","print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n","\n","# Create tensors (automatically assigned to GPU if available)\n","x = tf.constant([1.0, 2.0, 3.0])\n","y = tf.constant([4.0, 5.0, 6.0])\n","\n","# Perform computation\n","z = x + y\n","print(z)  # Output: tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n","\n","\n","\n","import torch\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Create a tensor and move it to GPU\n","x = torch.tensor([1.0, 2.0, 3.0]).to(device)\n","y = torch.tensor([4.0, 5.0, 6.0]).to(device)\n","\n","# Perform computation on GPU\n","z = x + y\n","print(z)  # Output: tensor([5., 7., 9.], device='cuda:0')\n","\n"],"metadata":{"id":"Sc9E1x70WPUB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745893287547,"user_tz":-330,"elapsed":483,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}},"outputId":"cd83c3ad-bf68-48b3-ce7f-7204a2a9dbe4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","tf.Tensor([5. 7. 9.], shape=(3,), dtype=float32)\n","Using device: cuda\n","tensor([5., 7., 9.], device='cuda:0')\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9BpgCQyAyFFT"},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["**AutoGrad Function**"],"metadata":{"id":"TvKJUAiV_uq-"}},{"cell_type":"code","source":["# The autograd package provides automatic differentiation\n","# for all operations on Tensors\n","\n","# requires_grad = True -> tracks all operations on the tensor.\n","x = torch.tensor([ 1.0156, -0.3449,  0.1732], requires_grad=True)\n","\n","y = x + 2\n","\n","# y was created as a result of an operation, so it has a grad_fn attribute.\n","# grad_fn: references a Function that has created the Tensor\n","print(x) # created by the user -> grad_fn is None\n","print(y)\n","print(y.grad_fn)\n","y.backward(torch.ones_like(y))\n","print(x.grad)\n","# Do more operations on y\n","z = y * y\n","print(z)\n","z = z.mean()\n","print(z)\n","\n","# Let's compute the gradients with backpropagation\n","# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n","# The gradient for this tensor will be accumulated into .grad attribute.\n","# It is the partial derivate of the function w.r.t. the tensor\n","z.backward(torch.ones_like(z))\n","print(x.grad) # dz/dx\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3PwRgHryGmE","outputId":"ab78ffa6-c750-49dc-ab06-d29ce0317918","executionInfo":{"status":"ok","timestamp":1745893381932,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.0156, -0.3449,  0.1732], requires_grad=True)\n","tensor([3.0156, 1.6551, 2.1732], grad_fn=<AddBackward0>)\n","<AddBackward0 object at 0x7c270a7a51e0>\n","tensor([1., 1., 1.])\n","tensor([9.0938, 2.7394, 4.7228], grad_fn=<MulBackward0>)\n","tensor(5.5187, grad_fn=<MeanBackward0>)\n","tensor([3.0104, 2.1034, 2.4488])\n"]}]},{"cell_type":"markdown","source":["**Stop a tensor from tracking history**"],"metadata":{"id":"ccjvi2cGwwJo"}},{"cell_type":"code","source":["# For example during our training loop when we want to update our weights\n","# then this update operation should not be part of the gradient computation\n","# - x.requires_grad_(False)\n","# - x.detach()\n","# - wrap in 'with torch.no_grad():"],"metadata":{"id":"VXynG2JCylhx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# .requires_grad_(...) changes an existing flag in-place.\n","a = torch.randn(2, )\n","print(a)\n","print(a.requires_grad)\n","b = ((a * 3) / (a - 1))[]\n","print(b.grad_fn)\n","a.requires_grad_(True)\n","print(a.requires_grad)\n","b = (a * a).sum()\n","print(b.requires_grad)\n","print(b)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZLi5Nyn49yLX","outputId":"d0fc1c2b-901a-4720-ea42-5fc66b331b97","executionInfo":{"status":"ok","timestamp":1745893656174,"user_tz":-330,"elapsed":13,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 1.8471, -0.5652])\n","False\n","None\n","True\n","True\n","tensor(3.7314, grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"code","source":["# .detach(): get a new Tensor with the same content but no gradient computation:\n","a = torch.tensor([1.0,2.0], requires_grad=True)\n","print(a.requires_grad)\n","# b = a.detach()\n","d=a*a\n","print(d)\n","# d=d.mean()\n","d.backward(torch.ones_like(d))\n","print(a.grad)\n","# c=torch.tensor([1,2],)\n","# print(b.requires_grad)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5j-lnIsD9667","outputId":"23a107b5-5c77-4bb3-b58f-ff56ed935d91","executionInfo":{"status":"ok","timestamp":1745470017063,"user_tz":-330,"elapsed":23,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","tensor([1., 4.], grad_fn=<MulBackward0>)\n","tensor([2., 4.])\n"]}]},{"cell_type":"code","source":["# wrap in 'with torch.no_grad():'\n","a = torch.randn(2, 2, requires_grad=True)\n","print(a.requires_grad)\n","with torch.no_grad():\n","    print((a ** 2).requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIHgGtxg-KY2","outputId":"eacf961a-195a-45e5-dee7-04cd62da6afd","executionInfo":{"status":"ok","timestamp":1745893838899,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"markdown","source":["**Emptying Gradients**"],"metadata":{"id":"eU11U9JUxDZ3"}},{"cell_type":"code","source":["# backward() accumulates the gradient for this tensor into .grad attribute.\n","# !!! We need to be careful during optimization !!!\n","# Use .zero_() to empty the gradients before a new optimization step!\n","weights = torch.ones(4, requires_grad=True)\n","\n","for epoch in range(3):\n","    # just a dummy example\n","    model_output = (weights*3).sum()\n","    model_output.backward()\n","\n","    print(weights.grad)\n","\n","    # optimize model, i.e. adjust weights...\n","    with torch.no_grad():\n","        weights -= 0.1 * weights.grad\n","\n","    # this is important! It affects the final weights & output\n","    weights.grad.zero_()\n","\n","print(weights)\n","print(model_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmV9-OLc-Q-C","outputId":"bcfbfab9-fd28-45e3-9ab7-cdf4694be502"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([3., 3., 3., 3.])\n","tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n","tensor(4.8000, grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"markdown","source":["**Backpropagation for Linear Regression Example**"],"metadata":{"id":"lZ8qPu2c_XRc"}},{"cell_type":"code","source":["import torch\n","\n","x = torch.tensor(1.0)\n","y = torch.tensor(2.0)\n","\n","# This is the parameter we want to optimize -> requires_grad=True\n","w = torch.tensor(1.0, requires_grad=True)\n","print(w)\n","# forward pass to compute loss\n","y_predicted = w * x\n","loss = (y_predicted - y)**2\n","print(loss)\n","\n","# backward pass to compute gradient dLoss/dw\n","loss.backward()\n","print(w.grad)\n","\n","# update weights\n","# next forward and backward pass...\n","\n","# continue optimizing:\n","# update weights, this operation should not be part of the computational graph\n","with torch.no_grad():\n","    w -= 0.01 * w.grad\n","# don't forget to zero the gradients\n","w.grad.zero_()\n","\n","# next forward and backward pass..."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SlUu0Qba-cMl","outputId":"07413b15-1407-45d0-fba9-0b592a760f04","executionInfo":{"status":"ok","timestamp":1745894469133,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1., requires_grad=True)\n","tensor(1., grad_fn=<PowBackward0>)\n","tensor(-2.)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["**Gradient Descent for Linear Regression (Manually)**\n","\n"],"metadata":{"id":"_tAd5BwHhb6f"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Compute every step manually\n","\n","# Linear regression\n","# f = w * x\n","\n","# here : f = 2 * x\n","X = np.array([1, 2, 3, 4], dtype=np.float32)\n","Y = np.array([2, 4, 6, 8], dtype=np.float32)\n","\n","w = 0.0\n","\n","# model output\n","def forward(x):\n","    return w * x\n","\n","# loss = MSE\n","def loss(y, y_pred):\n","    return ((y_pred - y)**2).mean()\n","\n","# J = MSE = 1/N * (w*x - y)**2\n","# dJ/dw = 1/N * 2x(w*x - y)\n","def gradient(x, y, y_pred):\n","    return np.mean(2*x*(y_pred - y))\n","\n","print(f'Prediction before training: f(5) = {forward(5):.3f}')\n","\n","# Training\n","learning_rate = 0.01\n","n_iters = 100\n","\n","for epoch in range(n_iters):\n","    # predict = forward pass\n","    y_pred = forward(X)\n","\n","    # loss\n","    l = loss(Y, y_pred)\n","\n","    # calculate gradients\n","    dw = gradient(X, Y, y_pred)\n","\n","    # update weights\n","    w =w- learning_rate * dw\n","\n","    if epoch % 10 == 0:\n","        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n","\n","print(f'Prediction after training: f(5) = {forward(5):.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cW2rniyjhl-M","outputId":"5040941b-90f2-46ea-9104-1de0b8725e5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch 1: w = 0.300, loss = 30.00000000\n","epoch 11: w = 1.665, loss = 1.16278565\n","epoch 21: w = 1.934, loss = 0.04506905\n","epoch 31: w = 1.987, loss = 0.00174685\n","epoch 41: w = 1.997, loss = 0.00006770\n","epoch 51: w = 1.999, loss = 0.00000262\n","epoch 61: w = 2.000, loss = 0.00000010\n","epoch 71: w = 2.000, loss = 0.00000000\n","epoch 81: w = 2.000, loss = 0.00000000\n","epoch 91: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n"]}]},{"cell_type":"markdown","source":["**Gradient Descent (Automatic)**"],"metadata":{"id":"itMv_ARRxoXd"}},{"cell_type":"code","source":["import torch\n","\n","# Here we replace the manually computed gradient with autograd\n","\n","# Linear regression\n","# f = w * x\n","\n","# here : f = 2 * x\n","X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n","Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n","\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","# model output\n","def forward(x):\n","    return w * x\n","\n","# loss = MSE\n","def loss(y, y_pred):\n","    return ((y_pred - y)**2).mean()\n","\n","print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n","\n","# Training\n","learning_rate = 0.01\n","n_iters = 100\n","\n","for epoch in range(n_iters):\n","    # predict = forward pass\n","    y_pred = forward(X)\n","\n","    # loss\n","    l = loss(Y, y_pred)\n","\n","    # calculate gradients = backward pass\n","    l.backward()\n","    dw=w.grad\n","\n","    # update weights\n","    #w.data = w.data - learning_rate * w.grad\n","    with torch.no_grad():\n","        w -= learning_rate * dw\n","\n","    # zero the gradients after updating\n","    w.grad.zero_()\n","\n","    if epoch % 10 == 0:\n","        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n","\n","print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmMMbK7ohom4","outputId":"438dca9a-2a7e-4c28-a6cd-4417d3c82be8","executionInfo":{"status":"ok","timestamp":1745894792857,"user_tz":-330,"elapsed":55,"user":{"displayName":"Sanidhya Sharma","userId":"11244205970451029434"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch 1: w = 0.300, loss = 30.00000000\n","epoch 11: w = 1.665, loss = 1.16278565\n","epoch 21: w = 1.934, loss = 0.04506890\n","epoch 31: w = 1.987, loss = 0.00174685\n","epoch 41: w = 1.997, loss = 0.00006770\n","epoch 51: w = 1.999, loss = 0.00000262\n","epoch 61: w = 2.000, loss = 0.00000010\n","epoch 71: w = 2.000, loss = 0.00000000\n","epoch 81: w = 2.000, loss = 0.00000000\n","epoch 91: w = 2.000, loss = 0.00000000\n","Prediction after training: f(5) = 10.000\n"]}]}]}