{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["TensorFlow and PyTorch are two of the most popular deep learning frameworks,\n","\n","•\tTensorFlow: Used in industry (Google, NVIDIA, Uber).\n","•\tPyTorch: More popular in research and academia (Facebook AI, OpenAI, Hugging Face).\n","\n","\n","•\tUse PyTorch if you want a more flexible, Pythonic, and research-friendly framework.\n","•\tUse TensorFlow if you need a production-ready framework with strong deployment capabilities.\n","\n"],"metadata":{"id":"RvsZDTNhgTmQ"}},{"cell_type":"markdown","source":["torchvision.datasets.MNIST is a built-in dataset loader in PyTorch that provides access to the MNIST dataset, which consists of 70,000 grayscale images of handwritten digits (0-9), each of size 28×28 pixels."],"metadata":{"id":"5XMwBtDvMd_M"}},{"cell_type":"markdown","source":["#Torchvision is a PyTorch package that provides datasets, model architectures, and image transformations for computer vision tasks.\n","#Torchvision includes popular datasets like MNIST, CIFAR10, ImageNet, and COCO."],"metadata":{"id":"uSgdfO_2fzvv"}},{"cell_type":"markdown","source":[],"metadata":{"id":"lGtfryy5fti-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2aNqzgs9xJRl"},"outputs":[],"source":["import torch\n","import torchvision\n","from torch.utils.data import Dataset,DataLoader\n","import torch.nn as nn"]},{"cell_type":"markdown","source":["1. **`import torch`**: This imports the core PyTorch library, which provides the fundamental building blocks for neural networks, including tensor operations, automatic differentiation, and neural network modules.\n","\n","2. **`import torchvision`**: This imports the `torchvision` library, which is an extension of PyTorch that provides datasets, model architectures, and image transformations commonly used in computer vision tasks.  It often simplifies loading common datasets and pre-trained models.\n","\n","3. **`from torch.utils.data import Dataset, DataLoader`**: This imports the `Dataset` and `DataLoader` classes from `torch.utils.data`.\n","    - `Dataset` is an abstract class representing a dataset. You typically create a custom subclass of `Dataset` to load and process your specific data.\n","    - `DataLoader` is a utility for efficiently loading data in batches during training.  It handles shuffling, batching, and parallel loading of data.\n","\n","4. **`import torch.nn as nn`**: This imports the `torch.nn` module, which provides building blocks for constructing neural networks.  It includes layers (e.g., linear, convolutional, recurrent), activation functions, loss functions, and other components necessary for defining the architecture of a neural network.\n"],"metadata":{"id":"xG9nzIq8MFwG"}},{"cell_type":"code","source":["train_set=torchvision.datasets.MNIST(root='/data',\n","                                     train=True,\n","                                     download=True,\n","                                     transform=torchvision.transforms.ToTensor()) #images in dataset are in array so converting it into tensor\n","test_set=torchvision.datasets.MNIST(root='/data',\n","                                     train=False,\n","                                     download=True,\n","                                    transform=torchvision.transforms.ToTensor())\n","\n","#backpropagation will work only if ur data is is in tensors"],"metadata":{"id":"f8oeXlKwxfgP","executionInfo":{"status":"ok","timestamp":1745868302984,"user_tz":-330,"elapsed":2879,"user":{"displayName":"Anubha Kapur","userId":"07972202054286695759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c75da3a-98d8-4fe1-cf69-0444eb76b0fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 58.6MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 40.1MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 12.9MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 8.60MB/s]\n"]}]},{"cell_type":"markdown","source":["train_set: Loads the training set of the MNIST dataset.\n","\n","root='/data': Specifies the directory where the dataset will be stored.  If the data is not present, it will be downloaded to this location.  You might need to adjust this path if your data is in another directory.\n","\n","train=True:  Indicates that this is the training dataset.\n","\n"," download=True: Automatically downloads the dataset if it's not already present at the specified root directory.\n","\n"," transform=torchvision.transforms.ToTensor(): Applies a transformation to each image in the dataset. torchvision.transforms.ToTensor() converts the images (which are typically PIL images) into PyTorch tensors, making them suitable for use with PyTorch models.  The tensors are scaled to have values in the range [0, 1].\n"],"metadata":{"id":"Ia0mrXUIOJog"}},{"cell_type":"code","source":["#Checking for GPU\n","device=torch.device('cuda' if torch.cuda.is_available() else 'cpu') #the device to use for computations (GPU if available, otherwise CPU).\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nREhNRpux6zt","outputId":"899adb67-b246-4ade-999d-e80fd37b0369","executionInfo":{"status":"ok","timestamp":1743756678969,"user_tz":-330,"elapsed":10,"user":{"displayName":"MAHAK GAMBHIR","userId":"18232796878054967565"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["train_loader=torch.utils.data.DataLoader(train_set,batch_size=32,num_workers=4,shuffle=True)\n","test_loader=torch.utils.data.DataLoader(test_set,batch_size=32,num_workers=4,shuffle=True)\n","# `num_workers=4`: Specifies that 4 worker processes will be used to load data in parallel. This can significantly speed up data loading, especially when dealing with large datasets.\n","# in test dataset, shuffling is not necessary. in train, its very important to shuffle"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MhGZYbWOyagz","outputId":"911e23dd-41ae-48f8-b704-7b8cff2b5e2a","executionInfo":{"status":"ok","timestamp":1743756917372,"user_tz":-330,"elapsed":70,"user":{"displayName":"MAHAK GAMBHIR","userId":"18232796878054967565"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["#Method 1"],"metadata":{"id":"Ll_0N9rTOqKn"}},{"cell_type":"code","source":["#just like we used to do in Keras\n","#here we dont have a Dense layer\n","#nn.Flatten(): flattens it into a single vector. For example, if your input is a 28x28 image, the nn.Flatten() layer will convert it into a vector of size 784 (28 * 28). This is necessary because fully connected layers (like nn.Linear) expect their input to be a single vector.\n","\n","\n","#nn.Linear(784, 512): This is a fully connected (or dense) layer. It takes the 784-dimensional input vector from the previous layer and applies a linear transformation to produce a 512-dimensional output vector. It has 784 input features and 512 output features. This layer learns a set of weights and biases to perform this transformation.\n","\n","\n","model=nn.Sequential(nn.Flatten(),\n","    nn.Linear(784,512),\n","    nn.ReLU(),\n","    nn.Linear(512,256),\n","    nn.ReLU(),\n","    nn.Linear(256,64),\n","    nn.ReLU(),\n","    nn.Linear(64,10)  #this is the output layer\n",")\n","model=torch.compile(model).to(device) # Compiles the model using torch.compile for potential performance gains (e.g. using TorchDynamo or AOTAutograd) and moves it to the specified device (GPU if available, otherwise CPU).\n","#it just initializes weights and biases"],"metadata":{"id":"hFvXSDI12H5B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["sequential and subclasses ... no functional API in pytorch\n","\n","cross entropy logits =True so no activation function is required in the output layer\n","\n","wintialization of weights and biases (compile)"],"metadata":{"id":"jax1V3PUCjTP"}},{"cell_type":"code","source":["#here we dont have model.compile, model.fit. Here we need to do custom training.we have to write it ourselves\n","optimizer=torch.optim.Adam(model.parameters(),lr=3e-4)\n","loss_fn=nn.CrossEntropyLoss()   #by default its sparse categorical"],"metadata":{"id":"9iLSRpTB2SYy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`model.parameters()`**: This crucial part passes all the trainable parameters (weights and biases) of your neural network model (`model`) to the optimizer.  The optimizer needs to know which values to adjust during training.  `model.parameters()` returns an iterator over all the tensors that have `requires_grad=True` in your model.\n"],"metadata":{"id":"CWKuOw05PEWD"}},{"cell_type":"code","source":["# prompt: what is loss_fn=nn.CrossEntropyLoss()\n","\n","# The loss function `nn.CrossEntropyLoss()` is used for multi-class classification.\n","# It combines a softmax layer (to produce class probabilities) and a negative log-likelihood loss.\n","# This is appropriate when your model's output is a vector of raw scores (logits) for each class.\n","# The softmax function converts these logits into probabilities, and the negative log-likelihood loss then measures the difference between the predicted probabilities and the true labels.\n","\n"],"metadata":{"id":"50TU0U8DoI5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If you needed binary cross-entropy, you'd use:\n","# loss_fn = nn.BCELoss()  # For probabilities (output should be between 0 and 1)\n","# or\n","# loss_fn = nn.BCEWithLogitsLoss()  # For logits (raw output from the last layer, no sigmoid applied)\n"],"metadata":{"id":"UNlB33AFDWBC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#No builtin function for accuracy over here. we have to write function for batch accuracy on our own. here accuracy is computed in batches\n","def batch_accuracy(output, y, N):\n","    pred = torch.argmax(output,dim=1)\n","    correct = (pred==y).sum().item()    # `.item()` extracts the value from the resulting tensor as a Python number\n","    return correct / N"],"metadata":{"id":"P0L4TYqa3QN2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function is designed to calculate the accuracy of your model's predictions on a batch of data. It's a common practice in machine learning to evaluate model performance in batches for efficiency. Here's a step-by-step explanation:\n","\n","**def batch_accuracy(output, y, N)::** This line defines the function named batch_accuracy and specifies its inputs:\n","\n","**output:** This is the output of your model for a given batch of data. It likely contains predicted probabilities or scores for each class.\n","\n","**y:** This represents the true labels (or ground truth) for the corresponding batch of data.\n","\n","**N:** This is the total number of samples in the batch.\n","\n","**pred =** torch.argmax(output, dim=1): This line calculates the predicted class labels for the batch:\n","\n","**torch.argmax:** This function finds the index of the maximum value along a specified dimension (dim=1 in this case, which corresponds to the class dimension). This effectively selects the class with the highest predicted probability or score for each sample in the batch.\n","pred: This variable now stores the predicted class labels for the batch.\n","correct = (pred == y).sum().item(): This line calculates the number of correct predictions in the batch:\n","\n","(pred == y): This comparison creates a Boolean tensor where True indicates a correct prediction and False indicates an incorrect prediction for each sample in the batch.\n",".sum(): This sums up all the True values in the Boolean tensor, effectively counting the number of correct predictions.\n",".item(): This extracts the single scalar value from the resulting tensor (the sum of correct predictions) and converts it into a standard Python number.\n","correct: This variable now holds the total number of correct predictions in the batch.\n","return correct / N: This line calculates and returns the accuracy of the batch:\n","\n","**It divides the number of correct predictions (correct) by the total number of samples in the batch (N).**\n","The result is the accuracy of the model's predictions for this specific batch, represented as a fraction (or percentage if multiplied by 100).\n","\n","In summary, this function takes the model's output, the true labels, and the batch size as input. It then determines the predicted labels, counts the correct predictions, and finally calculates and returns the accuracy for the batch. This accuracy value is essential for evaluating the performance of your model during training and testing."],"metadata":{"id":"PbbrFvd78VO2"}},{"cell_type":"code","source":["epochs=10\n","def training():\n","  for i in range(epochs):\n","    loss=0\n","    accuracy=0\n","    model.train()\n","    for indx,(image,label) in enumerate(train_loader):\n","      image=image.to(device)  # send data to GPU as we did for model\n","      label=label.to(device)\n","      output=model(image)\n","     #forward propagation till here\n","\n","      batch_loss=loss_fn(output,label)\n","      #backward propagation starts here\n","\n","      optimizer.zero_grad() #initially batch gradients are set to zero\n","#Its primary function is to reset the gradients of the model's parameters (weights and biases) to zero before starting the backpropagation for the next batch of data.\n","\n","      batch_loss.backward()  #It initiates the backpropagation process, which is the core of how neural networks learn.\n","# It calculates the gradients of the loss function with respect to all the model's parameters (weights and biases) that have requires_grad=True (meaning they are trainable).\n","\n","      optimizer.step()  # update parameters\n","      #It's the step where the optimizer actually updates the model's parameters (weights and biases) based on the gradients that were calculated during backpropagation (batch_loss.backward()).\n","\n","      loss+=batch_loss.item() #accumulate the loss values across multiple batches during the training process.\n","      accuracy+=batch_accuracy(output,label,label.shape[0])  ##label is a one-dimensional array. This gives number of samples in a batch, just like N\n","    print(f'Training Epoch: {i+1}, Accuracy: {accuracy/len(train_loader)}, Loss: {loss/len(train_loader)}')\n","\n","    #len(train_loader)=number of batches"],"metadata":{"id":"LgUO7fDh4bpC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**In PyTorch, gradients** are accumulated by default during backpropagation. This means that if you don't reset them to zero before processing a new batch, the gradients from the previous batch will be added to the gradients of the current batch. This can lead to incorrect weight updates and hinder the learning process.\n","\n","By calling **optimizer.zero_grad()**, you ensure that the gradients are calculated only for the current batch of data, allowing for proper weight updates."],"metadata":{"id":"eAseJexikMoZ"}},{"cell_type":"markdown","source":[],"metadata":{"id":"SbBuNPwKPb7-"}},{"cell_type":"markdown","source":["**In summary**: The loop goes through each batch of data in your training set. For each batch, it gives you:\n","\n","- `indx`: The batch number (starting from 0).\n","- `image`: A tensor containing the images in the batch.\n","- `label`: A tensor containing the correct labels for the images in the batch.\n"],"metadata":{"id":"A_M9DrM_JWdf"}},{"cell_type":"markdown","source":["`model.train()` sets the model to training mode.\n"," This is important because some layers, like dropout and batch normalization,\n"," behave differently during training and evaluation.  In training mode, these layers are active,\n","  while in evaluation mode (set using `model.eval()`), they are typically turned off or behave deterministically.\n","   Failing to switch to training mode can lead to inaccurate results and prevent proper learning during the training process.\n"],"metadata":{"id":"qKMCOEDvIaYk"}},{"cell_type":"markdown","source":["`training()`:  The main training loop.\n","    *   Iterates over epochs (passes over the entire training data).\n","    *   Iterates over batches of data from the training `DataLoader`.\n","    *   For each batch:\n","        1.  Moves data to the device.\n","        2.  Passes data through the model (`output = model(image)`).\n","        3.  Calculates the loss using the loss function.\n","        4.  Performs backpropagation (`batch_loss.backward()`), updating gradients.\n","        5.  Updates the model's weights using the optimizer (`optimizer.step()`).\n","        6.  Calculates the accuracy of the batch.\n","    *   Prints the average training loss and accuracy for each epoch."],"metadata":{"id":"PLubDZvMK1Yi"}},{"cell_type":"code","source":["def evaluate():\n","  accuracy=0\n","  loss=0\n","   # Sets the model to evaluation mode. This is important for layers like dropout and batch normalization that behave differently during training and evaluation.\n","  model.eval()\n","  # Disables gradient calculations. This is important during evaluation since gradients are not needed for inference. It saves memory and computational time.\n","  with torch.no_grad(): #no backpropagation is done here\n","    for (image,label) in test_loader:\n","      image=image.to(device)\n","      label=label.to(device)\n","      output=model(image)  # Performs a forward pass through the model to obtain the predicted output\n","\n","      batch_loss=loss_fn(output,label)\n","      accuracy+=batch_accuracy(output,label,label.shape[0]) #label is a one-dimensional array\n","      loss+=batch_loss.item()\n","    print(f'Test Accuracy: {accuracy*100/len(test_loader)}, Test Loss: {loss/len(test_loader)}')\n"],"metadata":{"id":"Rt8R5Lne7vXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZyFuvlyMK-t","executionInfo":{"status":"ok","timestamp":1743759141572,"user_tz":-330,"elapsed":12099,"user":{"displayName":"MAHAK GAMBHIR","userId":"18232796878054967565"}},"outputId":"ac356159-a363-4d5a-9779-00a235eca495"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["W0404 09:30:21.709000 233 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training Epoch: 1, Accuracy: 0.9007166666666667, Loss: 0.33793503912091255\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training Epoch: 2, Accuracy: 0.9629, Loss: 0.12633406118626395\n","Training Epoch: 3, Accuracy: 0.9751, Loss: 0.08113887280840426\n","Training Epoch: 4, Accuracy: 0.9825666666666667, Loss: 0.05728902567047626\n","Training Epoch: 5, Accuracy: 0.9869, Loss: 0.04198549642139114\n","Training Epoch: 6, Accuracy: 0.9900333333333333, Loss: 0.031191517790624252\n","Training Epoch: 7, Accuracy: 0.9921833333333333, Loss: 0.0238970316521707\n","Training Epoch: 8, Accuracy: 0.99325, Loss: 0.020613962910869546\n","Training Epoch: 9, Accuracy: 0.9957333333333334, Loss: 0.014239662292811166\n","Training Epoch: 10, Accuracy: 0.9950833333333333, Loss: 0.014465812071080048\n"]}]},{"cell_type":"code","source":["evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_Qy0WEBL1OZ","executionInfo":{"status":"ok","timestamp":1743759178088,"user_tz":-330,"elapsed":3794,"user":{"displayName":"MAHAK GAMBHIR","userId":"18232796878054967565"}},"outputId":"ca8a9b90-5b2a-4c41-919b-f794067ef651"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 98.01317891373802, Test Loss: 0.07694631563104506\n"]}]},{"cell_type":"markdown","source":["#2 Method:"],"metadata":{"id":"f2VGaQGnPjaZ"}},{"cell_type":"code","source":["\n","class first_nn(nn.Module):\n","  def __init__(self,input_size,hidden_size,num_classes):\n","    super(first_nn,self).__init__()\n","    self.flat=nn.Flatten()\n","    self.l1=nn.Linear(input_size,hidden_size)\n","    self.relu=nn.ReLU()\n","    self.l2=nn.Linear(hidden_size,hidden_size)\n","    self.l3=nn.Linear(hidden_size,num_classes)\n","  def forward(self,x):\n","    output=self.flat(x)\n","    output=self.l1(output)\n","    output=self.relu(output)\n","    output=self.l2(output)\n","    output=self.relu(output)\n","    output=self.l3(output)\n","    return output"],"metadata":{"id":"4TjIO1uBACHj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model1=first_nn(784,512,10)\n","model1=torch.compile(model1).to(device)"],"metadata":{"id":"qXSkNxD6EPRy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer=torch.optim.Adam(model1.parameters())\n","loss_fn=nn.CrossEntropyLoss()"],"metadata":{"id":"rMHN5OeVHNYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs=10\n","def training1():\n","  for i in range(epochs):\n","    loss=0\n","    accuracy=0\n","    model1.train()\n","    for (image,label) in train_loader:\n","      image=image.to(device)\n","      label=label.to(device)\n","\n","      output=model1(image)\n","\n","      batch_loss=loss_fn(output,label)\n","      optimizer.zero_grad()\n","      batch_loss.backward()\n","      optimizer.step()\n","      loss+=batch_loss.item()\n","      accuracy+=batch_accuracy(output,label,label.shape[0])\n","    print(f'Training Epoch: {i+1}, Accuracy: {accuracy/len(train_loader)}, Loss: {loss/len(train_loader)}')"],"metadata":{"id":"3lAqLBbbCMAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate1():\n","  accuracy=0\n","  loss=0\n","  model.eval()\n","  with torch.no_grad():\n","    for (image,label) in test_loader:\n","      image=image.to(device)\n","      label=label.to(device)\n","\n","      output=model1(image)\n","\n","      batch_loss=loss_fn(output,label)\n","      accuracy+=batch_accuracy(output,label,label.shape[0])\n","      loss+=batch_loss.item()\n","    print(f'Test Accuracy: {accuracy*100/len(test_loader)}, Test Loss: {loss/len(test_loader)}')\n"],"metadata":{"id":"-b_vV_xkCXL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate1()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XRPZuhLH2uG","outputId":"151e2ac2-5724-48b8-8d82-7022585f7dc5","executionInfo":{"status":"ok","timestamp":1743563796647,"user_tz":-330,"elapsed":336449,"user":{"displayName":"Maninder","userId":"09085528543087217757"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 99.6235341151386, Test Loss: 0.011436372119413927\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WMnXEmxbH4H_"},"execution_count":null,"outputs":[]}]}