{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dk55wKupv6id"},"outputs":[],"source":["import pandas as pd\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer # pre processing\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset,DataLoader\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n"]},{"cell_type":"code","source":["train_df=pd.read_csv('/content/Train.csv')\n","test_df=pd.read_csv('/content/Test.csv')"],"metadata":{"id":"4WD9fniJCMDK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RICWKourlO5_","executionInfo":{"status":"ok","timestamp":1745655562753,"user_tz":-330,"elapsed":32,"user":{"displayName":"JASMEET SINGH","userId":"00352875487003010804"}},"outputId":"64135d60-c707-47ab-96f2-e6fdb57e3558"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 40000 entries, 0 to 39999\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    40000 non-null  object\n"," 1   label   40000 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 625.1+ KB\n"]}]},{"cell_type":"code","source":["nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"id":"8okwzl4-0C8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745655565642,"user_tz":-330,"elapsed":120,"user":{"displayName":"JASMEET SINGH","userId":"00352875487003010804"}},"outputId":"63e6576b-edc2-4384-8d66-6aab0ada9b60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import re\n","stopwords_list=nltk.corpus.stopwords.words('english')\n","ws=WordNetLemmatizer()\n","def preprocessing(text):\n","  text=text.lower()\n","  text=re.sub(r'[^a-z\\s]',' ',text)  #all special symbols get removed\n","  no_stop=[word for word in text.split() if word not in stopwords_list]\n","  preprocessed=' '.join([ws.lemmatize(word) for word in no_stop])\n","  return preprocessed\n"],"metadata":{"id":"_chCpFcry8wB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df['text']=train_df['text'].apply(preprocessing) #applying preprocessing method on text column\n","test_df['text']=test_df['text'].apply(preprocessing)"],"metadata":{"id":"Ll3Uuvei0BE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer=Tokenizer(num_words=10000,oov_token='<oov>') #top 10k most frequent words are assigned index. words of test data not present in training set are assigned oov tokens\n","tokenizer.fit_on_texts(train_df['text']) #tokenizer is always trained on training data, not testing data"],"metadata":{"id":"IprtKB580S_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sequences=tokenizer.texts_to_sequences(train_df['text']) #sequence of words in the reviews are replaced by sequence of tokens\n","test_sequences=tokenizer.texts_to_sequences(test_df['text'])"],"metadata":{"id":"i7lABVLS1ZwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_sequences[0]"],"metadata":{"id":"ErICd3l4G539","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745655619903,"user_tz":-330,"elapsed":16,"user":{"displayName":"JASMEET SINGH","userId":"00352875487003010804"}},"outputId":"4267ec2c-d6b1-4f7a-e922-580ea171dc51"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1969,\n"," 364,\n"," 66,\n"," 1495,\n"," 6947,\n"," 2473,\n"," 249,\n"," 187,\n"," 157,\n"," 6947,\n"," 249,\n"," 5218,\n"," 249,\n"," 352,\n"," 1,\n"," 909,\n"," 5,\n"," 352,\n"," 1601,\n"," 7171,\n"," 757,\n"," 301,\n"," 595,\n"," 425,\n"," 144,\n"," 15,\n"," 3,\n"," 1242,\n"," 13,\n"," 12,\n"," 2252,\n"," 312,\n"," 144,\n"," 1,\n"," 1197,\n"," 196,\n"," 84,\n"," 7172,\n"," 408,\n"," 1674,\n"," 30,\n"," 1326,\n"," 114,\n"," 416,\n"," 6947,\n"," 2233,\n"," 293,\n"," 2086,\n"," 1642,\n"," 5,\n"," 560,\n"," 881,\n"," 55,\n"," 69,\n"," 5676,\n"," 107,\n"," 1,\n"," 2041,\n"," 201,\n"," 912,\n"," 1,\n"," 1,\n"," 251,\n"," 58,\n"," 2409,\n"," 195,\n"," 235,\n"," 4112,\n"," 329,\n"," 4,\n"," 1828,\n"," 1677,\n"," 1413,\n"," 823,\n"," 109,\n"," 3022,\n"," 8428,\n"," 1,\n"," 9654,\n"," 1,\n"," 1,\n"," 1,\n"," 510,\n"," 2520,\n"," 4918]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["train_padded_sequences=pad_sequences(train_sequences,maxlen=100,padding='post') #by default, its post padding. so we can even omit this\n","test_padded_sequences=pad_sequences(test_sequences,maxlen=100,padding='post')"],"metadata":{"id":"Og-5jR493UJ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["converting your padded sequences and labels into PyTorch tensors, which is necessary before feeding them into a PyTorch model."],"metadata":{"id":"uwYoumVaI6bI"}},{"cell_type":"code","source":["train_padded_torch=torch.tensor(train_padded_sequences,dtype=torch.long)  # converting numpy array to a torch tensor\n","test_padded_torch=torch.tensor(test_padded_sequences,dtype=torch.long)\n","\n","train_labels_torch=torch.tensor(train_df['label'].values,dtype=torch.float32)\n","test_labels_torch=torch.tensor(test_df['label'].values,dtype=torch.float32)\n"],"metadata":{"id":"1UNoNMby4Vi4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" You're now preparing your data to be used with a PyTorch DataLoader\n"," This creates PyTorch Dataset objects from your input features (padded sequences) and labels.\n","\n","TensorDataset is a convenient way to bundle your input and label tensors together so you can:[Itâ€™s a wrapper around tensors. It behaves like a dataset where each item is a tuple of tensors]\n","\n","Access them in pairs (input, label)\n","\n","Feed them easily into a DataLoader for batching, shuffling, etc."],"metadata":{"id":"b4hEgzPrJmFK"}},{"cell_type":"markdown","source":["question on batches"],"metadata":{"id":"Zkt6ASHX6u0u"}},{"cell_type":"code","source":["#now we need to create batches\n","train_dataset=TensorDataset(train_padded_torch,train_labels_torch)\n","test_dataset=TensorDataset(test_padded_torch,test_labels_torch)"],"metadata":{"id":"NqkyG-IO49DZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TyJ6zftYrATM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True) #faster optimization with large batch size\n","test_loader=DataLoader(test_dataset,batch_size=32,shuffle=True)  #small batch size gives better results. time will be more but curve would be smooth"],"metadata":{"id":"x_nBNdg15ImY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextClassification(nn.Module): #nn.Module is the base class in pytorch for all neural networks. model's name is TextClassification\n","# in init method, we are just defining the layers to be used in this NN model\n","  def __init__(self,vocab_size,embedding_dim,hidden_units):  #self is an object. we are adding 1 hidden layer so just 1 hidden units, if more hidden layers need to be added, then u may write hidden units1, hidden units2, etc\n","    super(TextClassification,self).__init__() #super constructor of this class TextClassification\n","    self.embedding=nn.Embedding(vocab_size,embedding_dim) # Embedding layer is a 2 layered FFNN just like word2vec.\n","    #Input to this layer is (batch size,sequence length), i.e. shape is (32,100). all the training egs will be of size (32,100)\n","    #output of this layer is (batch size, seq, embedding size) i.e. shape is (32,100,300)\n","    #each word will be replaced by a vector of 300 dimension\n","    self.global_avg_pool=nn.AdaptiveAvgPool1d(1)  # its just like Global Average pooling layer in TF. its converts word vectors to document vectors\n","    #Average of embeddings of all words in 1 document. each document shall be now represented by a 300 dimensional vector.\n","    #output shape of this layer is (32 x 300 x 1) (32 examples, each example is 300 dim ) bcoz for 100 words in the seq, we have just 1 vector of 300dim\n","    #it takes 3D input n gives 3d output\n","    self.fc1=nn.Linear(embedding_dim,hidden_units) #output of previous layer will serve as input here i.e. 300 dimensional vector so no. of input neurons is embedding_dim\n","    self.relu=nn.ReLU()\n","    self.fc2=nn.Linear(hidden_units,1)  # output layer\n","    self.sigmoid=nn.Sigmoid()\n","\n","  def forward(self,text): # pytorch forward propagation starts here. we define the sequence in which these layers will be connected\n","    embedded=self.embedding(text).permute(0,2,1) # previously output is of shape (32,100,300) but here we need output to be of shape (32,300,100) to pass to next global_avg_pool\n","    #so we need to swap last 2 dim, so for the avg pooling for the next layer, sequence length goes to last and 1st dim remains same so permute(0,2,1)\n","    pooled=self.global_avg_pool(embedded).squeeze(2) # average is to be done of all the words in the seq, i.e. on the last dim of (32,300,100),i.e. 2nd dim i.e, we need to do the average of all the words in the document, i.e. of all 100 words in the seq.\n","    # average pooling layer shape is (32,300,1), but next layer should have input shape as (32,300). every eg should be  a flatten 1 D array to pass to the next Linear layer. so we remove the 2nd dimension so we use squeeze(2)\n","    # In TF we need not use permute and squeeze\n","    hidden=self.relu(self.fc1(pooled))  #Applying relu on hidden layer\n","    output=self.sigmoid(self.fc2(hidden)) #Applying sigmoid on output layer\n","    return output\n"],"metadata":{"id":"UGhO4P5x5_uF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","vocab_size=10002 #1 is for padding n other for unknown OOV token\n","embedding_dim=300\n","hidden_units=256\n","model=TextClassification(vocab_size,embedding_dim,hidden_units).to(device) #model is an instance of TextClassification Model class"],"metadata":{"id":"HeBnXqk780bE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_fn=nn.BCELoss()    #binary cross entropy loss\n","optimizer=optim.Adam(model.parameters(),lr=3e-4)"],"metadata":{"id":"BH9FPGYt9LO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZzaXBpB1L8jP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def batch_accuracy(outputs,labels):\n","  predicted=(outputs>0.5).float()\n","  correct=(predicted==labels).sum().item()\n","  total=labels.size(0)\n","  return correct/total"],"metadata":{"id":"3f0_2hl79qEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs=10\n","for i in range(epochs):\n","  model.train()\n","  loss=0\n","  accuracy=0\n","  for texts,labels in train_loader:\n","    texts,labels=texts.to(device),labels.to(device)\n","    optimizer.zero_grad()\n","    outputs=model(texts).squeeze()  #probability will be returned, which is 1x1 so we squeeze it to convert it into a scalar value\n","    batch_loss=loss_fn(outputs,labels)\n","    batch_loss.backward()\n","    optimizer.step()\n","    loss+=batch_loss.item()\n","    accuracy+=batch_accuracy(outputs,labels)\n","  print(f'Epoch: {i+1}, Training_Accuracy: {accuracy/len(train_loader)}, Training_loss: {loss/len(train_loader)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JsJNfCgF-BTy","outputId":"921a860c-530b-478c-ef87-7346161b08e9","executionInfo":{"status":"ok","timestamp":1745655742475,"user_tz":-330,"elapsed":39589,"user":{"displayName":"JASMEET SINGH","userId":"00352875487003010804"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Training_Accuracy: 0.750775, Training_loss: 0.49339095450639725\n","Epoch: 2, Training_Accuracy: 0.860125, Training_loss: 0.3254195868849754\n","Epoch: 3, Training_Accuracy: 0.8835, Training_loss: 0.2791259401202202\n","Epoch: 4, Training_Accuracy: 0.89765, Training_loss: 0.25131699267625807\n","Epoch: 5, Training_Accuracy: 0.909025, Training_loss: 0.2290497768998146\n","Epoch: 6, Training_Accuracy: 0.915475, Training_loss: 0.21538467758893967\n","Epoch: 7, Training_Accuracy: 0.92035, Training_loss: 0.2016336963355541\n","Epoch: 8, Training_Accuracy: 0.92705, Training_loss: 0.18862017841339113\n","Epoch: 9, Training_Accuracy: 0.93005, Training_loss: 0.18122458250373602\n","Epoch: 10, Training_Accuracy: 0.935, Training_loss: 0.1729540966063738\n"]}]},{"cell_type":"code","source":["def evaluate():\n","  model.eval()\n","  loss=0\n","  accuracy=0\n","  with torch.no_grad():\n","    for texts,labels in test_loader:\n","      texts,labels=texts.to(device),labels.to(device)\n","      outputs=model(texts).squeeze()\n","      batch_loss=loss_fn(outputs,labels)\n","      loss+=batch_loss.item()\n","      accuracy+=batch_accuracy(outputs,labels)\n","  print(f'Testing_Accuracy: {accuracy/len(test_loader)}, Testing_loss: {loss/len(test_loader)}')"],"metadata":{"id":"gUQResd2BuFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1e8ERkNxComy","outputId":"d4afd617-c30a-4489-d8dc-df98d4005eb7","executionInfo":{"status":"ok","timestamp":1744350178790,"user_tz":-330,"elapsed":502,"user":{"displayName":"MAHAK GAMBHIR","userId":"18232796878054967565"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing_Accuracy: 0.8648487261146497, Testing_loss: 0.35987203150607977\n"]}]},{"cell_type":"markdown","source":["Accuracy is low so we need to do some hyper parameter tuning.\n","Moreover, NN is not a good technique for doing text classification. we need to use RNN for this."],"metadata":{"id":"SE_enqNzpKxB"}},{"cell_type":"code","source":["tokenizer.vocab_size"],"metadata":{"id":"WFHbKdl-Cpsk","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"error","timestamp":1745658518714,"user_tz":-330,"elapsed":6,"user":{"displayName":"JASMEET SINGH","userId":"00352875487003010804"}},"outputId":"8a14971c-6937-4b02-9608-967ace5f3491"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'Tokenizer' object has no attribute 'vocab_size'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-5d757b1e26b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'vocab_size'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZJAeErZJWZ2E"},"execution_count":null,"outputs":[]}]}